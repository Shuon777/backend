import logging
from typing import Dict, List, Optional, Tuple
from langchain_core.documents import Document
from collections import defaultdict
from .document_processing import find_resource_by_uri

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def format_response(search_results: List[Document], 
                  requested_types: Optional[List[str]] = None,
                  scores: Optional[List[float]] = None,
                  debug_mode: bool = False,
                  user_id: Optional[str] = None) -> Tuple[str, Dict[str, List[str]]]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ —á–∏—Ç–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç.
    
    Args:
        search_results: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        requested_types: –ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã —Ä–µ—Å—É—Ä—Å–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        scores: –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
        debug_mode: –§–ª–∞–≥ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ –∏–∑:
        - –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
        - –°–ª–æ–≤–∞—Ä—å —Å URL —Ä–µ—Å—É—Ä—Å–æ–≤:
            {
                "images": [—Å–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π],
                "files": [—Å–ø–∏—Å–æ–∫ URL —Ñ–∞–π–ª–æ–≤],
                "geo_places": [—Å–ø–∏—Å–æ–∫ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –º–µ—Å—Ç]
            }
    """
    resources = []
    urls = {"images": [], "files": [], "geo_places": []}
    
    special_cases = {
        "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è": "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
        "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ": "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
        "–¥–æ–∫—É–º–µ–Ω—Ç—ã": "–î–æ–∫—É–º–µ–Ω—Ç",
        "–¥–æ–∫—É–º–µ–Ω—Ç": "–î–æ–∫—É–º–µ–Ω—Ç",
        "–∞—É–¥–∏–æ": "–ê—É–¥–∏–æ",
        "–≤–∏–¥–µ–æ": "–í–∏–¥–µ–æ",
        "–≥—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã": "–ì—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã",
        "–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è": "–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
        "–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è": "–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
        "—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è": "–¢—Ä–∞–Ω—Å–ª—è—Ü–∏—è",
        "–≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏": "–í–Ω–µ—à–Ω—è—è —Å—Å—ã–ª–∫–∞",
        "—Å—Å—ã–ª–∫–∏": "–í–Ω–µ—à–Ω—è—è —Å—Å—ã–ª–∫–∞",
        "—Ç–µ–∫—Å—Ç": "–¢–µ–∫—Å—Ç"
    }

    def extract_description(page_content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        if not page_content:
            return ""
        lines = page_content.split('\n')
        for line in lines:
            if line.startswith("–û–ø–∏—Å–∞–Ω–∏–µ:"):
                return line.replace("–û–ø–∏—Å–∞–Ω–∏–µ:", "").strip()
        return page_content
    kart_found = False
    for i, doc in enumerate(search_results):
        meta = doc.metadata
        doc_type = meta.get("type", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø")
        description = extract_description(doc.page_content)
        resource_URI = meta.get("source", "Unknown")
        file_path = "/var/www/salut_bot/faiss_index_path/resources_dist.json"
        
        resource = find_resource_by_uri(file_path, resource_URI)
        content = ''
        
        if resource:
            doc_type = resource.get("type", doc_type)
            if doc_type == "–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è":
                if not kart_found:
                    kart_found = True
                    all_synonyms = resource.get('geo_synonyms', [])
                    
                    cleaned_synonyms = []
                    for synonym in all_synonyms:
                        cleaned = synonym.strip().strip('"').strip("'").strip()
                        if cleaned:
                            cleaned_synonyms.append(cleaned)
                    
                    if cleaned_synonyms:
                        urls['geo_places'] = cleaned_synonyms 
                        logger.debug(f'–û—á–∏—â–µ–Ω–Ω—ã–µ –≥–µ–æ-—Å–∏–Ω–æ–Ω–∏–º—ã: {cleaned_synonyms}')
            elif doc_type == "–¢–µ–∫—Å—Ç":
                content = resource.get('content', '')

        resource_data = {
            "type": doc_type,
            "name": meta.get("name", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
            "description": description,
            "content": content,
            "source": meta.get("file_path", "")
        }

        if debug_mode and scores and i < len(scores):
            resource_data["debug"] = {
                "similarity": round(scores[i], 4),
                "position": i + 1,
                "source_excerpt": doc.page_content[:100] + "..." if doc.page_content else "",
                "metadata": {k: v for k, v in meta.items() if not k.startswith('_')}
            }

        resources.append(resource_data)

        if meta.get("file_path"):
            if doc_type == "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ":
                urls["images"].append(meta["file_path"])
            else:
                urls["files"].append(meta["file_path"])

    normalized_requested = []
    if requested_types:
        for t in requested_types:
            normalized_t = special_cases.get(t.lower(), t)
            normalized_requested.append(normalized_t)
        
        filtered_resources = []
        for r in resources:
            if r["type"].lower() in [t.lower() for t in normalized_requested]:
                filtered_resources.append(r)
        resources = filtered_resources

    type_templates = {
      "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ": {
    "single": lambda r: (
        f"üì∑ {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}" +
        (f" - {r['description']}" if (user_id and user_id.startswith("telegram") and r.get('description')) else "") +
        (f"{' (—Å—Ö–æ–∂–µ—Å—Ç—å: ' + str(r['debug']['similarity']) + ')' if debug_mode and 'debug' in r else ''}")
    ).strip(),
    "multiple": lambda rs: (
        "\n".join(
            f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}" +
            (f" - {r['description']}" if (user_id and user_id.startswith("telegram") and r.get('description')) else "") +
            (f"{' (—Å—Ö–æ–∂–µ—Å—Ç—å: ' + str(r['debug']['similarity']) + ')' if debug_mode and 'debug' in r else ''}")
            for i, r in enumerate(rs)
        )
    ),
    "none": "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
},

    "–¢–µ–∫—Å—Ç": {
          "single": lambda r: (
              f"üìù {r.get('name', '–¢–µ–∫—Å—Ç–æ–≤—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª')}\n"
              f"{r.get('content', '')}\n"
          ),
        # "multiple": lambda rs: (
        #     "\n".join(
        #         f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}\n"
        #         f"{r.get('content', '')}\n"
        #         f"{' (—Å—Ö–æ–∂–µ—Å—Ç—å: ' + str(r['debug']['similarity']) + ')' if debug_mode and 'debug' in r else ''}"
        #         for i, r in enumerate(rs))
            
        # ),
        "multiple": lambda rs: (
            f"üìù{rs[0].get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}\n"
            f"{rs[0]['content'] if rs[0].get('content') else ''}"
            f"{' (—Å—Ö–æ–∂–µ—Å—Ç—å: ' + str(rs[0]['debug']['similarity']) + ')' if debug_mode and 'debug' in rs[0] else ''}"
        ),
        "none": "–¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
    },
      "–ê—É–¥–∏–æ": {
          "single": lambda r: (
              f"üéß {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
              f"{' - ' + r['description'] if r.get('description') else ''}"
              f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
          ).strip(),
          "multiple": lambda rs: (
              "üîä –ù–µ—Å–∫–æ–ª—å–∫–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:\n" +
              "\n".join(
                  f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
                  f"{' - ' + r['description'] if r.get('description') else ''}"
                  f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
                  for i, r in enumerate(rs))),
          "none": "–ê—É–¥–∏–æ—Ñ–∞–π–ª—ã –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
      },
      "–í–∏–¥–µ–æ": {
          "single": lambda r: (
              f"üé¨ {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
              f"{' - ' + r['description'] if r.get('description') else ''}"
              f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
          ).strip(),
          "multiple": lambda rs: (
              "üìπ –ù–µ—Å–∫–æ–ª—å–∫–æ –≤–∏–¥–µ–æ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:\n" +
              "\n".join(
                  f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
                  f"{' - ' + r['description'] if r.get('description') else ''}"
                  f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
                  for i, r in enumerate(rs))),
          "none": "–í–∏–¥–µ–æ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
      },
      "–î–æ–∫—É–º–µ–Ω—Ç": {
          "single": lambda r: (
              f"üìÑ {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
              f"{' - ' + r['description'] if r.get('description') else ''}"
              f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
          ).strip(),
          "multiple": lambda rs: (
              "üìÇ –ù–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:\n" +
              "\n".join(
                  f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
                  f"{' - ' + r['description'] if r.get('description') else ''}"
                  f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
                  for i, r in enumerate(rs))),
          "none": "–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
      },
      "–ì—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã": {
          "single": lambda r: (
              f"üìà {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
              f"{' - ' + r['description'] if r.get('description') else ''}"
              f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
          ).strip(),
          "multiple": lambda rs: (
              "üìä –ù–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:\n" +
              "\n".join(
                  f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
                  f"{' - ' + r['description'] if r.get('description') else ''}"
                  f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
                  for i, r in enumerate(rs))),
          "none": "–ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
      },
     "–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è": {
    "single": lambda r: (
        f"üó∫Ô∏è {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
        f"{' - ' + r['content'] if r.get('content') else ''}"
        f"{' (—Å—Ö–æ–∂–µ—Å—Ç—å: ' + str(r['debug']['similarity']) + ')' if debug_mode and 'debug' in r else ''}"
    ).strip(),
    "multiple": lambda rs: (
        f"üó∫Ô∏è {rs[0].get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
        f"{' - ' + rs[0]['content'] if rs[0].get('content') else ''}"
        f"{' (—Å—Ö–æ–∂–µ—Å—Ç—å: ' + str(rs[0]['debug']['similarity']) + ')' if debug_mode and 'debug' in rs[0] else ''}"
        if rs else "–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
    ),
    "none": "–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
},

      "–¢—Ä–∞–Ω—Å–ª—è—Ü–∏—è": {
          "single": lambda r: (
              f"üì° {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
              f"{' - ' + r['description'] if r.get('description') else ''}"
              f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
          ).strip(),
          "multiple": lambda rs: (
              "üì∫ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏:\n" +
              "\n".join(
                  f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
                  f"{' - ' + r['description'] if r.get('description') else ''}"
                  f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
                  for i, r in enumerate(rs))),
          "none": "–¢—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
      },
      "–í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏": {
          "single": lambda r: (
              f"üîó {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
              f"{' - ' + r['description'] if r.get('description') else ''}"
              f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
          ).strip(),
          "multiple": lambda rs: (
              "üìé –í–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:\n" +
              "\n".join(
                  f"{i+1}. {r.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}"
                  f"{' - ' + r['description'] if r.get('description') else ''}"
                  f"{' | –°—Å—ã–ª–∫–∞: ' + r['source'] if r.get('source') else ''}"
                  for i, r in enumerate(rs))),
          "none": "–í–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
      }
  }


    resources_by_type = defaultdict(list)
    for res in resources:
        res_type = res['type']
        resources_by_type[res_type].append(res)


    response_parts = []
    known_types = set(type_templates.keys())
    
    for res_type, res_list in resources_by_type.items():
        if res_type in known_types:
            template = type_templates[res_type]
            if len(res_list) == 1 or res_type=='–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è':
                response = template["single"](res_list[0])
            else:
                response = template["multiple"](res_list)
            #logger.debug(f'–®–∞–±–ª–æ–Ω –æ—Ç–≤–µ—Ç–∞:{template}')
            response_parts.append(response)

    if requested_types:
        for req_type in requested_types:
            normalized_req_type = special_cases.get(req_type.lower(), req_type)
            if normalized_req_type not in resources_by_type:
                template = type_templates.get(normalized_req_type, {}).get("none")
                
                if template:
                    response_parts.append(template)

    if not response_parts:
        return "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ", urls

    answer = "\n\n".join(response_parts)
    # if(answer=='–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'):
    #     answer='–ö–∞—Ä—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–∏–∂–µ:'
    if debug_mode:
        answer += "\n\n---\n–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n"
        answer += f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(search_results)}\n"
        if scores:
            answer += (
                f"–°—Ö–æ–∂–µ—Å—Ç—å: min={min(scores):.2f}, "
                f"max={max(scores):.2f}, "
                f"avg={sum(scores)/len(scores):.2f}\n"
            )
        answer += f"–ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã: {requested_types or '–í—Å–µ'}\n"
        answer += f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã: {list(resources_by_type.keys())}"
    logger.debug(f'–í–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç:{answer}')
    #logger.debug(f'–í–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–µ —Å—Å—ã–ª–∫–∏:{urls}')
    return answer, urls
